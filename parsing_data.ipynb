{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "228c3a87-a8a6-4841-b69f-2830f100bb3d",
   "metadata": {},
   "source": [
    "# Data Extraction using Python libaries\n",
    "\n",
    "## First step:\n",
    "\n",
    "In general, we need to extract data from the Wikipedia page on Highest-Grossing Films.\n",
    "In the beginning, we need to import and install all needed libaries and upgrade some features: we wll use libary BeautifulSoup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ac8234-0760-4848-8c2b-34745a140498",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install requests beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075be8bb-5f8d-4ade-a1fe-13077da44458",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a58e5b-2c30-419e-974e-b1e7345298da",
   "metadata": {},
   "source": [
    "In this section, we will import all the necessary libraries for working with data.\n",
    "\n",
    "### Libraries to be imported:\n",
    "- **requests**: For making HTTP requests.\n",
    "- **beautifulsoup4**: For parsing HTML and XML documents.\n",
    "- **pandas**: For working with tabular data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "42a8c43b-38ea-4c18-a9f7-f8dbb4a11dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad22a6d-b30b-4e21-9a15-9d9d2d7563f3",
   "metadata": {},
   "source": [
    "### Parsing the Highest-Grossing Films Table from Wikipedia\n",
    "\n",
    "In this section, we will parse the table of highest-grossing films from Wikipedia using the `requests` and `BeautifulSoup` libraries.\n",
    "\n",
    "### Steps:\n",
    "1. **Fetch the webpage**: We use the `requests` library to download the HTML content of the Wikipedia page.\n",
    "2. **Parse the HTML**: We use `BeautifulSoup` to parse the HTML and extract the table.\n",
    "3. **Locate the table**: We identify the table by its class name and store it in a variable for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e58fe7ef-6b11-4820-82a3-1a382684c0e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table found!\n"
     ]
    }
   ],
   "source": [
    "# URL of the Wikipedia page\n",
    "url = \"https://en.wikipedia.org/wiki/List_of_highest-grossing_films\"\n",
    "\n",
    "# Fetch the webpage content\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content using BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Find all tables on the page\n",
    "tables = soup.find_all('table')\n",
    "# Locate the specific table by its class name\n",
    "table = soup.find('table', {'class': 'wikitable sortable plainrowheaders sticky-header col4right col5center col6center'})\n",
    "\n",
    "# Check if the table was found\n",
    "if table:\n",
    "    print(\"Table found!\")\n",
    "else:\n",
    "    print(\"Table not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d7a1de-372d-4c95-b6a7-7479a5206bfe",
   "metadata": {},
   "source": [
    "### Extracting Director and Country Information from a Movie Page\n",
    "\n",
    "This function extracts information about the director(s) and country/countries of origin from the infobox of a movie's Wikipedia page.\n",
    "\n",
    "### Function: `get_info_using_link(file_link)`\n",
    "- **Input**: A URL (`file_link`) pointing to the Wikipedia page of a movie.\n",
    "- **Output**: A tuple containing:\n",
    "  - `directors`: A list of directors (if found).\n",
    "  - `origin_countries`: A list of countries of origin (if found).\n",
    "\n",
    "### Steps:\n",
    "1. **Fetch the webpage**: The function uses the `requests` library to download the HTML content of the movie's Wikipedia page.\n",
    "2. **Parse the HTML**: The `BeautifulSoup` library is used to parse the HTML and locate the infobox table.\n",
    "3. **Extract information**: The function searches for the \"Directed by\" and \"Country\"/\"Countries\" rows in the infobox and extracts the relevant data.\n",
    "4. **Clean the data**: The function removes any unwanted characters (e.g., `[1]`, `[2]`) from the extracted data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "bdd07c72-9414-4ac8-92c3-c15a0b13ffa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info_using_link(file_link):\n",
    "    response = requests.get(file_link)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    infobox = soup.find('table', {'class': 'infobox vevent'})\n",
    "    \n",
    "    directors = None\n",
    "    origin_countries = None\n",
    "    \n",
    "    if infobox:\n",
    "        for row in infobox.find_all('tr'):\n",
    "            header = row.find('th')\n",
    "            if header and 'Directed by' in header.text:\n",
    "                td = row.find('td')\n",
    "                if td.find_all('li'): \n",
    "                    directors = [li.get_text(strip=True) for li in td.find_all('li')]\n",
    "                    directors = [director.split('[')[0].strip() for director in directors]\n",
    "                else:    \n",
    "                    directors = [row.find('td').text.strip()]\n",
    "            if header and 'Countries' in header.text:\n",
    "                countries_td = row.find('td')\n",
    "                origin_countries = [li.get_text(strip=True) for li in countries_td.find_all('li')]\n",
    "                origin_countries = [country.split('[')[0].strip() for country in origin_countries]\n",
    "            if header and 'Country' in header.text:\n",
    "                origin_countries = [row.find('td').text.strip()]\n",
    "\n",
    "    return directors, origin_countries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6ad0e4-796a-4699-95d6-f184cb9ee009",
   "metadata": {},
   "source": [
    "### Parsing and Processing Wikipedia Data\n",
    "\n",
    "This code parses a Wikipedia table containing information about movies to extract information about rank, title of film, year and box office.  \n",
    "\n",
    "### Steps:\n",
    "\n",
    "1. **Extracting table rows** – `find_all('tr')`.\n",
    "2. **Filtering rows** – The first row (`rows[1:]`) is skipped as it contains headers.\n",
    "3. **Parsing data**:\n",
    "   - `rank` and `peak`: extracted from the first two columns.\n",
    "   - `title` and `link`: found in the header cell (`th`).\n",
    "   - `box_office`: cleaned by removing `<sup>` tags.\n",
    "   - `year`: the release year of the movie.\n",
    "4. **Fetching additional data** – The function `get_info_using_link()` is used to retrieve directors and production countries via the movie's Wikipedia link.\n",
    "5. **Building the `data` list** – The final dataset is structured for further processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e8674333-68c9-4fb5-808d-39ea0130e377",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = table.find_all('tr')\n",
    "data = []\n",
    "\n",
    "for row in rows[1:]:\n",
    "    cols = row.find_all('td')\n",
    "    if len(cols) > 4:\n",
    "        rank = int(cols[0].text.strip())\n",
    "        peak = int(cols[1].find(string=True, recursive=False).strip())\n",
    "        title = row.find('th').find('a').text.strip()\n",
    "        link = row.find('th').find('a')['href']\n",
    "        for sup in cols[2].find_all('sup'):\n",
    "            sup.decompose()\n",
    "        box_office = cols[2].text.strip()\n",
    "        year = cols[3].text.strip()\n",
    "        directors, origin_countries = get_info_using_link(\"https://en.wikipedia.org/\" + link)\n",
    "        data.append([rank, peak, title, directors, box_office, origin_countries, year])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe4d824-d461-48d0-9880-d48cd829518d",
   "metadata": {},
   "source": [
    "### Cleaning and Updating the Film Data\n",
    "\n",
    "In this section, we perform the following operations on the `film_data` DataFrame:\n",
    "\n",
    "1. **Create a DataFrame**: We create a DataFrame from the provided data with columns: `rank`, `peak`, `title`, `director`, `box_office`, `country`, and `release_year`.\n",
    "2. **Clean the `box_office` column**: We remove the `[TFB]` suffix from the `box_office` values using regex.\n",
    "3. **Update specific rows due to different issues with html code structure of Wikipedia pages about films**:\n",
    "   - Set the `country` for the film at index 12 to `['United States', 'China']`.\n",
    "   - Remove the first director from the `director` list for the film at index 29.\n",
    "   - Set the `director` for the film at index 33 to `['Anna Boden', 'Ryan Fleck']`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8d6b9fda-9bbd-4bdf-b288-2cedd4241d43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aldir\\AppData\\Local\\Temp\\ipykernel_23892\\3505874508.py:4: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  film_data['country'][12] = ['United States', 'China']\n",
      "C:\\Users\\aldir\\AppData\\Local\\Temp\\ipykernel_23892\\3505874508.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  film_data['country'][12] = ['United States', 'China']\n",
      "C:\\Users\\aldir\\AppData\\Local\\Temp\\ipykernel_23892\\3505874508.py:6: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  film_data['director'][33] = ['Anna Boden', 'Ryan Fleck']\n",
      "C:\\Users\\aldir\\AppData\\Local\\Temp\\ipykernel_23892\\3505874508.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  film_data['director'][33] = ['Anna Boden', 'Ryan Fleck']\n"
     ]
    }
   ],
   "source": [
    "film_data = pd.DataFrame(data, columns=['rank', 'peak', 'title', 'director', 'box_office', 'country',\n",
    "       'release_year'])\n",
    "film_data['box_office'] = film_data['box_office'].replace('[TFB]', '', regex=True)\n",
    "film_data['country'][12] = ['United States', 'China']\n",
    "film_data['director'][29].pop(0)\n",
    "film_data['director'][33] = ['Anna Boden', 'Ryan Fleck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "22981f31-1cdc-46e4-a5a8-bc6d648aa10f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   rank  peak                         title                    director  \\\n",
      "0     1     1                        Avatar             [James Cameron]   \n",
      "1     2     1             Avengers: Endgame  [Anthony Russo, Joe Russo]   \n",
      "2     3     3      Avatar: The Way of Water             [James Cameron]   \n",
      "3     4     1                       Titanic             [James Cameron]   \n",
      "4     5     3  Star Wars: The Force Awakens              [J. J. Abrams]   \n",
      "\n",
      "       box_office                          country release_year  \n",
      "0  $2,923,706,026  [United Kingdom, United States]         2009  \n",
      "1  $2,797,501,328                  [United States]         2019  \n",
      "2  $2,320,250,281                  [United States]         2022  \n",
      "3  $2,257,844,554                  [United States]         1997  \n",
      "4  $2,068,223,624                  [United States]         2015  \n"
     ]
    }
   ],
   "source": [
    "#just a cell to show first 5 rows of table 'film_data'\n",
    "print(film_data.head(5)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9ea1e7-fde3-4206-856d-636ab538e05a",
   "metadata": {},
   "source": [
    "### Installing Required Libraries\n",
    "\n",
    "In this section, we install the necessary libraries for working with PostgreSQL and data manipulation.\n",
    "\n",
    "### Libraries to be installed:\n",
    "- **psycopg2-binary**: A PostgreSQL adapter for Python.\n",
    "- **pandas**: For data manipulation and analysis.\n",
    "- **sqlalchemy**: For interacting with databases using Python.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f3ab7e-0e27-4e59-89ca-da7483f486d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install psycopg2-binary pandas sqlalchemy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346d3113-b0a7-492a-b172-8b517339275c",
   "metadata": {},
   "source": [
    "### Loading Data into PostgreSQL\n",
    "\n",
    "In this section, we prepare the `film_data` DataFrame and upload it to a PostgreSQL database.\n",
    "\n",
    "### Steps:\n",
    "1. **Prepare the data**:\n",
    "   - Convert the `director` and `country` columns from lists to strings (if they are lists).\n",
    "2. **Set up the database connection**:\n",
    "   - Create a connection string using the `sqlalchemy` library.\n",
    "3. **Upload the data**:\n",
    "   - Use the `to_sql` method to upload the DataFrame to the `films` table in the PostgreSQL database.\n",
    "\n",
    "### Database connection details:\n",
    "- **Username**: `postgres`\n",
    "- **Password**: `Jloaj098890`\n",
    "- **Host**: `localhost`\n",
    "- **Port**: `5432`\n",
    "- **Database**: `film_data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "98ecf5cf-f43f-427d-829b-0bdecdc213bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data is successfully uploaded in PostgreSQL!\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "\n",
    "# Convert 'director' and 'country' columns from lists to strings\n",
    "film_data['director'] = film_data['director'].apply(lambda x: ', '.join(x) if isinstance(x, list) else x)\n",
    "film_data['country'] = film_data['country'].apply(lambda x: ', '.join(x) if isinstance(x, list) else x)\n",
    "\n",
    "# Database connection details\n",
    "username = 'postgres'\n",
    "password = 'Jloaj098890'\n",
    "host = 'localhost'\n",
    "port = 5432\n",
    "database = 'film_data'\n",
    "\n",
    "# Create the connection string\n",
    "connection_string = f'postgresql+psycopg2://{username}:{password}@{host}:{port}/{database}'\n",
    "\n",
    "# Create the SQLAlchemy engine\n",
    "engine = create_engine(connection_string)\n",
    "\n",
    "# Upload the DataFrame to the PostgreSQL database\n",
    "film_data.to_sql('films', engine, if_exists='append', index=False)\n",
    "\n",
    "print(\"Data is successfully uploaded in PostgreSQL!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1acc4356-08fb-45a2-a46a-d9a04b19471c",
   "metadata": {},
   "source": [
    "### Extracting Data from PostgreSQL and Saving to JSON\n",
    "\n",
    "In this section, we perform the following steps to extract data from a PostgreSQL table and save it to a JSON file:\n",
    "\n",
    "### Steps:\n",
    "1. **Connect to the PostgreSQL database**: We use the `psycopg2` library to establish a connection to the `film_data` database.\n",
    "\n",
    "2. **Fetch data from the `films` table**: We execute a SQL query (`SELECT * FROM films`) to retrieve all rows from the `films` table.\n",
    "\n",
    "3. **Save the data to a JSON file**: We use the `json` module to serialize the data and save it to a file named `films.json`.\n",
    "\n",
    "4. **Close the database connection**: After completing the operations, we close the cursor and the database connection to free up resources.\n",
    "\n",
    "### JSON File Structure:\n",
    "The resulting JSON file (`films.json`) will contain an array of objects, where each object represents a film with the following fields:\n",
    "- `rank`\n",
    "- `peak`\n",
    "- `title`\n",
    "- `release_year`\n",
    "- `director`\n",
    "- `box_office`\n",
    "- `country`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "5857ded9-465a-41f7-b9d2-b101b010ea8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been successfully exported to 'films.json'.\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "import json\n",
    "\n",
    "# Connect to the PostgreSQL database\n",
    "conn = psycopg2.connect(\n",
    "    dbname=\"film_data\",\n",
    "    user=\"postgres\",\n",
    "    password=\"Jloaj098890\",\n",
    "    host=\"localhost\",\n",
    "    port=\"5432\"\n",
    ")\n",
    "\n",
    "# Fetch data from the 'films' table\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(\"SELECT * FROM films\")\n",
    "rows = cursor.fetchall()\n",
    "\n",
    "# Convert rows to a list of dictionaries\n",
    "data = []\n",
    "for row in rows:\n",
    "    data.append({\n",
    "        \"rank\": row[1],\n",
    "        \"peak\": row[2],\n",
    "        \"title\": row[3],\n",
    "        \"release_year\": row[4],\n",
    "        \"director\": row[5],\n",
    "        \"box_office\": row[6],\n",
    "        \"country\": row[7]\n",
    "    })\n",
    "\n",
    "# Save the data to a JSON file\n",
    "with open(\"films.json\", \"w\") as f:\n",
    "    json.dump(data, f, indent=4)\n",
    "\n",
    "# Close the connection\n",
    "cursor.close()\n",
    "conn.close()\n",
    "\n",
    "print(\"Data has been successfully exported to 'films.json'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
